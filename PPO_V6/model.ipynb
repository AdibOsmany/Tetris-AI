{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INSTALLATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install gym\n",
    "# !pip3 install easyocr\n",
    "# !pip3 install torch torchvision torchaudio\n",
    "# !pip3 install stable-baselines3\n",
    "# !pip3 install shimmy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BACK END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box, Dict\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import easyocr\n",
    "import os\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import pyautogui as pag\n",
    "from PIL import Image\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join('Training', 'Logs')\n",
    "save_Path = os.path.join('Training', 'Saved Models', 'PPO_Model_Tetris')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def readImg(img):\n",
    "    reader=easyocr.Reader(['en'], gpu=False, verbose=False)\n",
    "    result=reader.readtext(img)\n",
    "    if len(result)==0:\n",
    "        return 0\n",
    "    elif (not result[0][1].isdigit()):\n",
    "        return 0\n",
    "    output=result[0][1]\n",
    "    return int(output)\n",
    "    \n",
    "#get the score \n",
    "def getScore():\n",
    "    pag.screenshot('score.png',region=(626,875, 300, 28))\n",
    "    return readImg('score.png')\n",
    "    \n",
    "#of lines\n",
    "def getLine():\n",
    "    pag.screenshot('line.png',region=(626,903, 300, 28))\n",
    "    return readImg('line.png')\n",
    "\n",
    "#of Blocks\n",
    "def getBlocks():\n",
    "    pag.screenshot('blocks.png',region=(626,931, 300, 28))\n",
    "    return readImg('blocks.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_BLOCK = (227,91,2)\n",
    "J_BLOCK = (33,65,198)\n",
    "T_BLOCK = (175,41,138)\n",
    "O_BLOCK = (227,159,2)\n",
    "S_BLOCK = (89,177,1)\n",
    "Z_BLOCK = (215,15,55)\n",
    "\n",
    "BLOCKS=[J_BLOCK,L_BLOCK,O_BLOCK,T_BLOCK,S_BLOCK,Z_BLOCK]\n",
    "\n",
    "#rotate counter clock wise\n",
    "def rotateCC():\n",
    "    pag.keyDown('z')\n",
    "    pag.keyUp('z')\n",
    "\n",
    "\n",
    "#rotate clockwise\n",
    "def rotateCW():\n",
    "    pag.keyDown('up')\n",
    "    pag.keyUp('up')\n",
    "\n",
    "\n",
    "#soft drop\n",
    "def softDrop():\n",
    "    pag.keyDown('down')\n",
    "    pag.keyUp('down')\n",
    "\n",
    "\n",
    "#hard drop\n",
    "def hardDrop():\n",
    "    pag.keyDown('space')\n",
    "    pag.keyUp('space')\n",
    "\n",
    "\n",
    "#move right\n",
    "def right():\n",
    "    pag.keyDown('right')\n",
    "    pag.keyUp('right')\n",
    "\n",
    "\n",
    "\n",
    "#move left\n",
    "def left():\n",
    "    pag.keyDown('left')\n",
    "    pag.keyUp('left')\n",
    "\n",
    "\n",
    "#new game\n",
    "#Point(x=913, y=670)   \n",
    "def newGame():\n",
    "    pag.FAILSAFE=True\n",
    "    pag.moveTo(913,670)\n",
    "    pag.click()\n",
    "\n",
    "\n",
    "def checkDone():\n",
    "    return (not pag.pixelMatchesColor(915,650,(0,0,0)))\n",
    "\n",
    "\n",
    "def getTetromino(piece = 1):\n",
    "    tetromino= pag.pixel(869,108+(piece*100))\n",
    "    if tetromino in BLOCKS:\n",
    "        return BLOCKS.index(tetromino) + 1\n",
    "    return 7\n",
    "\n",
    "    # No Piece [J_BLOCK, L_BLOCK, O_BLOCK, T_BLOCK, S_BLOCK, Z_BLOCK, I_BLOCK]\n",
    "    #   0         1        2        3        4        5        6        7   \n",
    "\n",
    "\n",
    "#hold key\n",
    "def hold():\n",
    "    pag.keyDown('c')\n",
    "    pag.keyUp('c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CANVAS=[(465, 145), (497, 145), (529, 145), (561, 145), (593, 145), (625, 145), (657, 145), (689, 145), (721, 145), (753, 145), (465, 177), (497, 177), (529, 177), (561, 177), (593, 177), (625, 177), (657, 177), (689, 177), (721, 177), (753, 177), (465, 209), (497, 209), (529, 209), (561, 209), (593, 209), (625, 209), (657, 209), (689, 209), (721, 209), (753, 209), (465, 241), (497, 241), (529, 241), (561, 241), (593, 241), (625, 241), (657, 241), (689, 241), (721, 241), (753, 241), (465, 273), (497, 273), (529, 273), (561, 273), (593, 273), (625, 273), (657, 273), (689, 273), (721, 273), (753, 273), (465, 305), (497, 305), (529, 305), (561, 305), (593, 305), (625, 305), (657, 305), (689, 305), (721, 305), (753, 305), (465, 337), (497, 337), (529, 337), (561, 337), (593, 337), (625, 337), (657, 337), (689, 337), (721, 337), (753, 337), (465, 369), (497, 369), (529, 369), (561, 369), (593, 369), (625, 369), (657, 369), (689, 369), (721, 369), (753, 369), (465, 401), (497, 401), (529, 401), (561, 401), (593, 401), (625, 401), (657, 401), (689, 401), (721, 401), (753, 401), (465, 433), (497, 433), (529, 433), (561, 433), (593, 433), (625, 433), (657, 433), (689, 433), (721, 433), (753, 433), (465, 465), (497, 465), (529, 465), (561, 465), (593, 465), (625, 465), (657, 465), (689, 465), (721, 465), (753, 465), (465, 497), (497, 497), (529, 497), (561, 497), (593, 497), (625, 497), (657, 497), (689, 497), (721, 497), (753, 497), (465, 529), (497, 529), (529, 529), (561, 529), (593, 529), (625, 529), (657, 529), (689, 529), (721, 529), (753, 529), (465, 561), (497, 561), (529, 561), (561, 561), (593, 561), (625, 561), (657, 561), (689, 561), (721, 561), (753, 561), (465, 593), (497, 593), (529, 593), (561, 593), (593, 593), (625, 593), (657, 593), (689, 593), (721, 593), (753, 593), (465, 625), (497, 625), (529, 625), (561, 625), (593, 625), (625, 625), (657, 625), (689, 625), (721, 625), (753, 625), (465, 657), (497, 657), (529, 657), (561, 657), (593, 657), (625, 657), (657, 657), (689, 657), (721, 657), (753, 657), (465, 689), (497, 689), (529, 689), (561, 689), (593, 689), (625, 689), (657, 689), (689, 689), (721, 689), (753, 689), (465, 721), (497, 721), (529, 721), (561, 721), (593, 721), (625, 721), (657, 721), (689, 721), (721, 721), (753, 721), (465, 753), (497, 753), (529, 753), (561, 753), (593, 753), (625, 753), (657, 753), (689, 753), (721, 753), (753, 753)]\n",
    "\n",
    "def pixel_rgb(img, x, y):\n",
    "\tr, g, b = img.getpixel((x, y))\n",
    "\treturn (r, g, b)\n",
    "\n",
    "def boardState():\n",
    "    img = pag.screenshot().convert('RGB')\n",
    "    return np.reshape([0 if pixel_rgb(img, i[0], i[1])==(0,0,0) else 1 for i in CANVAS],(20,10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate reward\n",
    "\n",
    "def calcReward(board):\n",
    "    reward=0\n",
    "    for i in board:\n",
    "        total=sum(i)\n",
    "        if ((total==0) or (total>7)):\n",
    "            reward+=1\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tetrisEnv(gym.Env):\n",
    "\n",
    "    global ACTIONS\n",
    "    #           0        1        2       3    4    5      6\n",
    "    ACTIONS=[rotateCC,rotateCW,softDrop,right,left,hold,hardDrop]\n",
    "\n",
    "    # No Piece [J_BLOCK, L_BLOCK, O_BLOCK, T_BLOCK, S_BLOCK, Z_BLOCK, I_BLOCK]\n",
    "    #   0         1        2        3        4        5        6        7   \n",
    "    \n",
    "    def __init__(self):\n",
    "        self.action_space = Discrete(7)\n",
    "        self.observation_space = Dict({'Current': Discrete(8), 'Next':Discrete(8), 'Hold':Discrete(8), 'Board':Box(0,1, shape=(20,10), dtype= int)})\n",
    "        self.state=OrderedDict(sorted({'Current': 0, 'Next': 0, 'Hold': 0, 'Board': np.reshape([[0]*10]*20, (20,10))}.items()))\n",
    "        self.empty=True\n",
    "        self.held=False\n",
    "        self.nextPiece2=0\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        reward=1\n",
    "\n",
    "        #apply actions\n",
    "        ACTIONS[action]()\n",
    "        if action==5:\n",
    "            if self.empty:\n",
    "                self.state['Hold']=self.state['Current']\n",
    "                self.state['Current']=self.state['Next']\n",
    "                self.state['Next']=getTetromino()\n",
    "                self.nextPiece2=getTetromino(piece=2)\n",
    "                self.empty=False\n",
    "                self.held=True\n",
    "\n",
    "            elif self.held==False:\n",
    "                temp=self.state['Current']\n",
    "                self.state['Current']=self.state['Hold']\n",
    "                self.state['Hold']=temp\n",
    "                self.held=True\n",
    "        \n",
    "        #update board\n",
    "        self.state['Board']=boardState()\n",
    "\n",
    "        #update current and next piece\n",
    "        next=getTetromino()\n",
    "        next2=getTetromino(piece=2)\n",
    "        if ((self.state['Next'] != next) or (self.nextPiece2 != next2)): #checks if piece changed\n",
    "            self.state['Current']=self.state['Next']\n",
    "            self.state['Next']=next\n",
    "            self.nextPiece2=next2\n",
    "            self.held=False\n",
    "            reward+=5\n",
    "        \n",
    "\n",
    "        #check if done\n",
    "        done=checkDone()\n",
    "        if done:\n",
    "            reward+= (getLine()*50)+5\n",
    "\n",
    "        #set placeholder\n",
    "        info={}\n",
    "\n",
    "        #return stuff\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "        \n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        newGame()\n",
    "        self.state= {'Current': getTetromino(), 'Hold': 0}\n",
    "        self.empty=True\n",
    "        self.held=False\n",
    "        time.sleep(2.5)\n",
    "        self.state['Board']=boardState()\n",
    "        self.state['Next']= getTetromino()\n",
    "        self.nextPiece2=getTetromino(piece=2)\n",
    "        self.state=OrderedDict(sorted(self.state.items()))\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env= tetrisEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# episodes=1\n",
    "# for i in range(episodes):\n",
    "#     done=False\n",
    "#     score=0   \n",
    "#     state=env.reset()\n",
    "#     while (not done):\n",
    "#         action= env.action_space.sample()\n",
    "#         state, reward, done, info = env.step(action)\n",
    "#         score+=reward\n",
    "#     print(\"Episode \", end='')\n",
    "#     print(str(i+1)+': ', end='')\n",
    "#     print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCEMENT LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for first time training\n",
    "\n",
    "# model = PPO('MultiInputPolicy', env, verbose=1, tensorboard_log=log_path)\n",
    "# model.learn(total_timesteps=1000)\n",
    "# model.save(save_Path)\n",
    "# del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tetrisEnv.reset() got an unexpected keyword argument 'seed'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO\u001b[38;5;241m.\u001b[39mload(save_Path, env\u001b[38;5;241m=\u001b[39menv)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m----> 4\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2500\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     model\u001b[38;5;241m.\u001b[39msave(save_Path)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m model\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stable_baselines3\\ppo\\ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:264\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfOnPolicyAlgorithm,\n\u001b[0;32m    255\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    260\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    261\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfOnPolicyAlgorithm:\n\u001b[0;32m    262\u001b[0m     iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 264\u001b[0m     total_timesteps, callback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_learn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    265\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    272\u001b[0m     callback\u001b[38;5;241m.\u001b[39mon_training_start(\u001b[38;5;28mlocals\u001b[39m(), \u001b[38;5;28mglobals\u001b[39m())\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stable_baselines3\\common\\base_class.py:423\u001b[0m, in \u001b[0;36mBaseAlgorithm._setup_learn\u001b[1;34m(self, total_timesteps, callback, reset_num_timesteps, tb_log_name, progress_bar)\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reset_num_timesteps \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    422\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 423\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m    424\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_episode_starts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mnum_envs,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[0;32m    425\u001b[0m     \u001b[38;5;66;03m# Retrieve unnormalized observation for saving into the buffer\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:77\u001b[0m, in \u001b[0;36mDummyVecEnv.reset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m     76\u001b[0m     maybe_options \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptions\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options[env_idx]} \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options[env_idx] \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m---> 77\u001b[0m     obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_seeds\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmaybe_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_obs(env_idx, obs)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Seeds and options are only used once\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stable_baselines3\\common\\monitor.py:83\u001b[0m, in \u001b[0;36mMonitor.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected you to pass keyword argument \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m into reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_reset_info[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: tetrisEnv.reset() got an unexpected keyword argument 'seed'"
     ]
    }
   ],
   "source": [
    "#overnight training\n",
    "model = PPO.load(save_Path, env=env)\n",
    "for i in range(100):\n",
    "    model.learn(total_timesteps=2500)\n",
    "    model.save(save_Path)\n",
    "    del model\n",
    "    model = PPO.load(save_Path, env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save & Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "model.save(save_Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete model\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "model = PPO.load(save_Path, env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_policy(model,env,n_eval_episodes=10, render=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(save_Path, env=env)\n",
    "episodes=1\n",
    "for i in range(episodes):\n",
    "    done=False\n",
    "    score=0   \n",
    "    obs=env.reset()\n",
    "    while not done:\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
